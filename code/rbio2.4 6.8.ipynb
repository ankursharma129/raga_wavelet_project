{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008ed29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 14:53:48.005974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:48.006006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "import threading\n",
    "import python_speech_features\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import pickle\n",
    "from os import listdir\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.layers import Dropout\n",
    "from os.path import isfile, join\n",
    "from keras.layers import Flatten, Conv1D, Input, MaxPooling1D, Conv2D, MaxPooling2D, BatchNormalization, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "import pywt\n",
    "from pywt import dwt, wavedec\n",
    "from os.path import isfile, join\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb230ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df,\n",
    "                 batch_size, validation,\n",
    "                 shuffle=True):\n",
    "\n",
    "        self.df = df.copy()\n",
    "        if validation:\n",
    "            self.df = self.df[25000:]\n",
    "        else:\n",
    "            self.df = self.df[:25000]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path):\n",
    "    \n",
    "        objects = []\n",
    "        with (open(path, \"rb\")) as openfile:\n",
    "            while True:\n",
    "                try:\n",
    "                    objects.append(pickle.load(openfile))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        return objects[0][\"audio\"]\n",
    "\n",
    "    \n",
    "    def __get_output(self, tag):\n",
    "        return lab[tag]\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        files = batches[\"file\"]\n",
    "        tag = batches[\"tag\"]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x) for x in files])\n",
    "\n",
    "        y_batch = np.asarray([self.__get_output(y) for y in tag])\n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7577f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv('dataGenrbio2.4.csv')\n",
    "dat = []\n",
    "files = dft[\"file\"]\n",
    "for i in files:\n",
    "    if i.split(\"/\")[-2] not in dat:\n",
    "        dat.append(i.split(\"/\")[-2])\n",
    "\n",
    "lab = {}\n",
    "\n",
    "for i in range(84):\n",
    "    tag = dat[i]\n",
    "    y_label = np.zeros((84))\n",
    "    y_label[i] = 1\n",
    "    lab[tag] = y_label\n",
    "    \n",
    "less = 0\n",
    "good = 0\n",
    "window_size = 2048\n",
    "hop_size =256\n",
    "allLabels = []\n",
    "sample_rate = 44100\n",
    "\n",
    "def AttRNNSpeechModel(rnn_func=L.LSTM):\n",
    "    inputs = Input(shape=(7, 20, 216))\n",
    "    x = L.Conv2D(128, (5, 1), activation='relu', padding='same')(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(64, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(32, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(8, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "\n",
    "    x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
    "\n",
    "    x = L.Bidirectional(rnn_func(64, return_sequences=True)\n",
    "                        )(x)\n",
    "    x = L.Bidirectional(rnn_func(32, return_sequences=True)\n",
    "                        )(x)\n",
    "\n",
    "    xFirst = L.Lambda(lambda q: q[:, -1])(x)\n",
    "    query = L.Dense(64)(xFirst)\n",
    "\n",
    "    # dot product attention\n",
    "    attScores = L.Dot(axes=[1, 2])([query, x])\n",
    "    attScores = L.Softmax(name='attSoftmax')(attScores)\n",
    "\n",
    "    # rescale sequence\n",
    "    attVector = L.Dot(axes=[1, 1])([attScores, x])\n",
    "\n",
    "    x = L.Dense(64, activation='relu')(attVector)\n",
    "    x = L.Dense(32, activation='relu')(attVector)\n",
    "    x = L.Dense(16)(x)\n",
    "\n",
    "    output = L.Dense(84, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = AttRNNSpeechModel()#, rnn_func=L.LSTM)\n",
    "\n",
    "model.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.00001) \n",
    "dft = shuffle(dft)\n",
    "training_generator = CustomDataGen(dft, 32, False)\n",
    "validation_generator = CustomDataGen(dft, 32, True)\n",
    "mc = ModelCheckpoint('best_modelrbio2.4.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "history=model.fit_generator(generator=training_generator, validation_data = validation_generator, epochs=50, callbacks=[mc,es], use_multiprocessing=True, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76420a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 14:53:54.114341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114496: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2021-12-04 14:53:54.114739: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-04 14:53:54.115004: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 7, 20, 216)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 7, 20, 128)   138368      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 7, 20, 128)  512         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 20, 64)    41024       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 7, 20, 64)   256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 20, 32)    10272       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 7, 20, 32)   128         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 7, 20, 16)    2576        ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 7, 20, 16)   64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 7, 20, 8)     648         ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 7, 20, 8)    32          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 7, 20, 1)     41          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 7, 20, 1)    4           ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " squeeze_last_dim (Lambda)      (None, 7, 20)        0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 7, 128)       43520       ['squeeze_last_dim[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 7, 64)       41216       ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 64)           0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           4160        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 7)            0           ['dense[0][0]',                  \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " attSoftmax (Softmax)           (None, 7)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 64)           0           ['attSoftmax[0][0]',             \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 84)           1428        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 286,857\n",
      "Trainable params: 286,359\n",
      "Non-trainable params: 498\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234161/3695000936.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(generator=training_generator, validation_data = validation_generator, epochs=50, callbacks=[mc,es], use_multiprocessing=True, workers=6)\n",
      "2021-12-04 14:53:59.630683: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 64 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 23068672 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 0s - loss: 2.7365 - accuracy: 0.2947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 14:55:17.919093: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2100 num_cores: 64 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 23068672 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49628, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 92s 110ms/step - loss: 2.7365 - accuracy: 0.2947 - val_loss: 1.7568 - val_accuracy: 0.4963\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 1.2665 - accuracy: 0.6256\n",
      "Epoch 00002: val_accuracy improved from 0.49628 to 0.69511, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 94s 119ms/step - loss: 1.2665 - accuracy: 0.6256 - val_loss: 0.9812 - val_accuracy: 0.6951\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.7925 - accuracy: 0.7476\n",
      "Epoch 00003: val_accuracy improved from 0.69511 to 0.78319, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.7925 - accuracy: 0.7476 - val_loss: 0.6553 - val_accuracy: 0.7832\n",
      "Epoch 4/50\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.8062\n",
      "Epoch 00004: val_accuracy improved from 0.78319 to 0.81558, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 99s 125ms/step - loss: 0.5835 - accuracy: 0.8060 - val_loss: 0.5318 - val_accuracy: 0.8156\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.8383\n",
      "Epoch 00005: val_accuracy improved from 0.81558 to 0.83501, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 98s 124ms/step - loss: 0.4731 - accuracy: 0.8383 - val_loss: 0.4783 - val_accuracy: 0.8350\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.8628\n",
      "Epoch 00006: val_accuracy improved from 0.83501 to 0.84359, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 99s 126ms/step - loss: 0.4018 - accuracy: 0.8628 - val_loss: 0.4869 - val_accuracy: 0.8436\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.8798\n",
      "Epoch 00007: val_accuracy improved from 0.84359 to 0.85848, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 100s 126ms/step - loss: 0.3507 - accuracy: 0.8798 - val_loss: 0.4106 - val_accuracy: 0.8585\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8952\n",
      "Epoch 00008: val_accuracy improved from 0.85848 to 0.87694, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 95s 120ms/step - loss: 0.3033 - accuracy: 0.8952 - val_loss: 0.3727 - val_accuracy: 0.8769\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9064\n",
      "Epoch 00009: val_accuracy did not improve from 0.87694\n",
      "781/781 [==============================] - 99s 125ms/step - loss: 0.2681 - accuracy: 0.9064 - val_loss: 0.3872 - val_accuracy: 0.8690\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.9121\n",
      "Epoch 00010: val_accuracy improved from 0.87694 to 0.87889, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 96s 121ms/step - loss: 0.2546 - accuracy: 0.9121 - val_loss: 0.3745 - val_accuracy: 0.8789\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9202\n",
      "Epoch 00011: val_accuracy improved from 0.87889 to 0.89006, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 103s 130ms/step - loss: 0.2303 - accuracy: 0.9202 - val_loss: 0.3333 - val_accuracy: 0.8901\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9284\n",
      "Epoch 00012: val_accuracy improved from 0.89006 to 0.89524, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.2069 - accuracy: 0.9284 - val_loss: 0.3243 - val_accuracy: 0.8952\n",
      "Epoch 13/50\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1949 - accuracy: 0.9323\n",
      "Epoch 00013: val_accuracy improved from 0.89524 to 0.90107, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 96s 120ms/step - loss: 0.1948 - accuracy: 0.9323 - val_loss: 0.3078 - val_accuracy: 0.9011\n",
      "Epoch 14/50\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.9384\n",
      "Epoch 00014: val_accuracy did not improve from 0.90107\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.1793 - accuracy: 0.9384 - val_loss: 0.3178 - val_accuracy: 0.8982\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9431\n",
      "Epoch 00015: val_accuracy improved from 0.90107 to 0.90301, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.1674 - accuracy: 0.9431 - val_loss: 0.3097 - val_accuracy: 0.9030\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9465\n",
      "Epoch 00016: val_accuracy improved from 0.90301 to 0.91354, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 98s 123ms/step - loss: 0.1513 - accuracy: 0.9465 - val_loss: 0.2762 - val_accuracy: 0.9135\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9529\n",
      "Epoch 00017: val_accuracy did not improve from 0.91354\n",
      "781/781 [==============================] - 96s 121ms/step - loss: 0.1365 - accuracy: 0.9529 - val_loss: 0.3220 - val_accuracy: 0.9056\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9458\n",
      "Epoch 00018: val_accuracy did not improve from 0.91354\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.1586 - accuracy: 0.9458 - val_loss: 0.3136 - val_accuracy: 0.9051\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9561\n",
      "Epoch 00019: val_accuracy did not improve from 0.91354\n",
      "781/781 [==============================] - 105s 133ms/step - loss: 0.1288 - accuracy: 0.9561 - val_loss: 0.3375 - val_accuracy: 0.8969\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9555\n",
      "Epoch 00020: val_accuracy improved from 0.91354 to 0.91386, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.1287 - accuracy: 0.9555 - val_loss: 0.2784 - val_accuracy: 0.9139\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9621\n",
      "Epoch 00021: val_accuracy improved from 0.91386 to 0.92390, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 101s 127ms/step - loss: 0.1093 - accuracy: 0.9621 - val_loss: 0.2437 - val_accuracy: 0.9239\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9602\n",
      "Epoch 00022: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 101s 128ms/step - loss: 0.1143 - accuracy: 0.9602 - val_loss: 0.3569 - val_accuracy: 0.9011\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9628\n",
      "Epoch 00023: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 103s 129ms/step - loss: 0.1085 - accuracy: 0.9628 - val_loss: 0.2867 - val_accuracy: 0.9174\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9629\n",
      "Epoch 00024: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.1082 - accuracy: 0.9629 - val_loss: 0.2899 - val_accuracy: 0.9155\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9648\n",
      "Epoch 00025: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.1001 - accuracy: 0.9648 - val_loss: 0.2823 - val_accuracy: 0.9218\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9671\n",
      "Epoch 00026: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.0939 - accuracy: 0.9671 - val_loss: 0.3210 - val_accuracy: 0.9090\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9727\n",
      "Epoch 00027: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 99s 125ms/step - loss: 0.0825 - accuracy: 0.9727 - val_loss: 0.3508 - val_accuracy: 0.9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9693\n",
      "Epoch 00028: val_accuracy did not improve from 0.92390\n",
      "781/781 [==============================] - 101s 127ms/step - loss: 0.0916 - accuracy: 0.9693 - val_loss: 0.2828 - val_accuracy: 0.9174\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9749\n",
      "Epoch 00029: val_accuracy improved from 0.92390 to 0.93070, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 96s 121ms/step - loss: 0.0773 - accuracy: 0.9749 - val_loss: 0.2374 - val_accuracy: 0.9307\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9722\n",
      "Epoch 00030: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.0820 - accuracy: 0.9722 - val_loss: 0.3260 - val_accuracy: 0.9124\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9722\n",
      "Epoch 00031: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 101s 128ms/step - loss: 0.0799 - accuracy: 0.9722 - val_loss: 0.2817 - val_accuracy: 0.9226\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9738\n",
      "Epoch 00032: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 97s 121ms/step - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.2764 - val_accuracy: 0.9242\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9776\n",
      "Epoch 00033: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 98s 123ms/step - loss: 0.0684 - accuracy: 0.9776 - val_loss: 0.2813 - val_accuracy: 0.9257\n",
      "Epoch 34/50\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9788\n",
      "Epoch 00034: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 101s 127ms/step - loss: 0.0621 - accuracy: 0.9788 - val_loss: 0.3695 - val_accuracy: 0.9095\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9708\n",
      "Epoch 00035: val_accuracy did not improve from 0.93070\n",
      "781/781 [==============================] - 100s 125ms/step - loss: 0.0858 - accuracy: 0.9708 - val_loss: 0.2598 - val_accuracy: 0.9302\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9790\n",
      "Epoch 00036: val_accuracy improved from 0.93070 to 0.93119, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 97s 121ms/step - loss: 0.0650 - accuracy: 0.9790 - val_loss: 0.2642 - val_accuracy: 0.9312\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9791\n",
      "Epoch 00037: val_accuracy did not improve from 0.93119\n",
      "781/781 [==============================] - 104s 131ms/step - loss: 0.0614 - accuracy: 0.9791 - val_loss: 0.2801 - val_accuracy: 0.9296\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9782\n",
      "Epoch 00038: val_accuracy did not improve from 0.93119\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.0674 - accuracy: 0.9782 - val_loss: 0.2620 - val_accuracy: 0.9307\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9842\n",
      "Epoch 00039: val_accuracy improved from 0.93119 to 0.94058, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 97s 122ms/step - loss: 0.0478 - accuracy: 0.9842 - val_loss: 0.2361 - val_accuracy: 0.9406\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9814\n",
      "Epoch 00040: val_accuracy did not improve from 0.94058\n",
      "781/781 [==============================] - 97s 121ms/step - loss: 0.0557 - accuracy: 0.9814 - val_loss: 0.4770 - val_accuracy: 0.8897\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9794\n",
      "Epoch 00041: val_accuracy did not improve from 0.94058\n",
      "781/781 [==============================] - 101s 127ms/step - loss: 0.0627 - accuracy: 0.9794 - val_loss: 0.2571 - val_accuracy: 0.9326\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9830\n",
      "Epoch 00042: val_accuracy did not improve from 0.94058\n",
      "781/781 [==============================] - 94s 117ms/step - loss: 0.0541 - accuracy: 0.9830 - val_loss: 0.2981 - val_accuracy: 0.9260\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9855\n",
      "Epoch 00043: val_accuracy did not improve from 0.94058\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 0.2717 - val_accuracy: 0.9339\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9839\n",
      "Epoch 00044: val_accuracy did not improve from 0.94058\n",
      "781/781 [==============================] - 101s 126ms/step - loss: 0.0487 - accuracy: 0.9839 - val_loss: 0.3531 - val_accuracy: 0.9182\n",
      "Epoch 45/50\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9778\n",
      "Epoch 00045: val_accuracy improved from 0.94058 to 0.94171, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 100s 126ms/step - loss: 0.0684 - accuracy: 0.9779 - val_loss: 0.2456 - val_accuracy: 0.9417\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9869\n",
      "Epoch 00046: val_accuracy improved from 0.94171 to 0.94252, saving model to best_modelrbio6.8.hdf5\n",
      "781/781 [==============================] - 101s 127ms/step - loss: 0.0391 - accuracy: 0.9869 - val_loss: 0.2464 - val_accuracy: 0.9425\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9806\n",
      "Epoch 00047: val_accuracy did not improve from 0.94252\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.0636 - accuracy: 0.9806 - val_loss: 0.2441 - val_accuracy: 0.9373\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9856\n",
      "Epoch 00048: val_accuracy did not improve from 0.94252\n",
      "781/781 [==============================] - 103s 129ms/step - loss: 0.0436 - accuracy: 0.9856 - val_loss: 0.2330 - val_accuracy: 0.9386\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9881\n",
      "Epoch 00049: val_accuracy did not improve from 0.94252\n",
      "781/781 [==============================] - 96s 121ms/step - loss: 0.0380 - accuracy: 0.9881 - val_loss: 0.2656 - val_accuracy: 0.9388\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9854\n",
      "Epoch 00050: val_accuracy did not improve from 0.94252\n",
      "781/781 [==============================] - 99s 124ms/step - loss: 0.0445 - accuracy: 0.9854 - val_loss: 0.2824 - val_accuracy: 0.9341\n"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv('dataGenrbio6.8.csv')\n",
    "dat = []\n",
    "files = dft[\"file\"]\n",
    "for i in files:\n",
    "    if i.split(\"/\")[-2] not in dat:\n",
    "        dat.append(i.split(\"/\")[-2])\n",
    "\n",
    "lab = {}\n",
    "\n",
    "for i in range(84):\n",
    "    tag = dat[i]\n",
    "    y_label = np.zeros((84))\n",
    "    y_label[i] = 1\n",
    "    lab[tag] = y_label\n",
    "    \n",
    "less = 0\n",
    "good = 0\n",
    "window_size = 2048\n",
    "hop_size =256\n",
    "allLabels = []\n",
    "sample_rate = 44100\n",
    "\n",
    "def AttRNNSpeechModel(rnn_func=L.LSTM):\n",
    "    inputs = Input(shape=(7, 20, 216))\n",
    "    x = L.Conv2D(128, (5, 1), activation='relu', padding='same')(inputs)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(64, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(32, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(8, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "\n",
    "    x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
    "\n",
    "    x = L.Bidirectional(rnn_func(64, return_sequences=True)\n",
    "                        )(x)\n",
    "    x = L.Bidirectional(rnn_func(32, return_sequences=True)\n",
    "                        )(x)\n",
    "\n",
    "    xFirst = L.Lambda(lambda q: q[:, -1])(x)\n",
    "    query = L.Dense(64)(xFirst)\n",
    "\n",
    "    # dot product attention\n",
    "    attScores = L.Dot(axes=[1, 2])([query, x])\n",
    "    attScores = L.Softmax(name='attSoftmax')(attScores)\n",
    "\n",
    "    # rescale sequence\n",
    "    attVector = L.Dot(axes=[1, 1])([attScores, x])\n",
    "\n",
    "    x = L.Dense(64, activation='relu')(attVector)\n",
    "    x = L.Dense(32, activation='relu')(attVector)\n",
    "    x = L.Dense(16)(x)\n",
    "\n",
    "    output = L.Dense(84, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = AttRNNSpeechModel()#, rnn_func=L.LSTM)\n",
    "\n",
    "model.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.00001) \n",
    "dft = shuffle(dft)\n",
    "training_generator = CustomDataGen(dft, 32, False)\n",
    "validation_generator = CustomDataGen(dft, 32, True)\n",
    "mc = ModelCheckpoint('best_modelrbio6.8.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "history=model.fit_generator(generator=training_generator, validation_data = validation_generator, epochs=50, callbacks=[mc,es], use_multiprocessing=True, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de85baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
