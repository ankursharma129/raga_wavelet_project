{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dac076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "import threading\n",
    "import python_speech_features\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import pickle\n",
    "from os import listdir\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77faf75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workDir = \"/Volumes/ANKUR'S/Data for audio/voxcelebDataset\"\n",
    "trainDir = \"free-spoken-digit-dataset-master/recordings/\"\n",
    "testDir = \"\"\n",
    "trainFiles = [f for f in listdir(trainDir) if isfile(join(trainDir, f))]\n",
    "# testFiles = [f for f in listdir(testDir) if isfile(join(testDir, f))]\n",
    "print(len(trainFiles))\n",
    "# print(len(testFiles))\n",
    "# metaFile = \"FSDKaggle2018.meta/train_post_competition.csv\"\n",
    "# meta = pd.read_csv(metaFile)\n",
    "\n",
    "def returnMelCoeff(signal, sr):\n",
    "    return librosa.feature.mfcc(signal, sr)\n",
    "\n",
    "def windowHann(signal, fr, windowSize):\n",
    "    window = get_window(\"hann\", windowSize)\n",
    "    windowed = signal * window\n",
    "    return windowed\n",
    "\n",
    "def padSignal(signal, sr, windowSize, hopSize):\n",
    "    signalLength = len(signal)\n",
    "    numZeros = windowSize - (signalLength - math.floor(signalLength/hopSize)*hopSize);\n",
    "    return np.concatenate((signal, np.zeros(numZeros)), axis=None)\n",
    "\n",
    "def process(signal, sr, windowSize, hopSize):\n",
    "    origLength = len(signal)\n",
    "    signal = padSignal(signal, sr, windowSize, hopSize)\n",
    "    mfccArray = []\n",
    "    for i in range(0, origLength, hopSize):\n",
    "        toProcess = windowHann(signal[i: i+windowSize], sr, windowSize)\n",
    "        mfccs = python_speech_features.mfcc(toProcess, sr, winlen=0.092, nfft = 2048, winstep=0.092)\n",
    "        mfccArray.append(mfccs[0].tolist())\n",
    "    return mfccArray\n",
    "\n",
    "windowSize = 2048\n",
    "hopSize = 256\n",
    "\n",
    "with open('dumpfileMNIST.pickle', 'wb') as handle:\n",
    "    for i in trainFiles:\n",
    "        fName = os.path.join(trainDir, i)\n",
    "        signal, sr = librosa.load(fName)\n",
    "        mfccArray = process(signal, sr, windowSize, hopSize)\n",
    "        tag = i\n",
    "        a = {tag: mfccArray}\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Done with: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = (2561, 1)\n",
    "\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(SimpleRNN(units = 20, activation='tanh', return_sequences=True, input_shape= inputShape))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 20, activation='tanh', return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 20, activation='tanh', return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 20))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "regressor.summary()\n",
    "\n",
    "# regressor.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "# buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e300dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickeledData = []\n",
    "objects = []\n",
    "with (open(\"MNIST_MFCC_Normal_20.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 512\n",
    "hopSize = 128\n",
    "trainDir = \"free-spoken-digit-dataset-master/recordings/\"\n",
    "trainFiles = [f for f in listdir(trainDir) if isfile(join(trainDir, f))]\n",
    "\n",
    "def windowHann(signal, fr, windowSize):\n",
    "    window = get_window(\"hann\", windowSize)\n",
    "    windowed = signal * window\n",
    "    return windowed\n",
    "\n",
    "def padSignal(signal, sr, windowSize, hopSize):\n",
    "    signalLength = len(signal)\n",
    "    numZeros = windowSize - (signalLength - math.floor(signalLength/hopSize)*hopSize);\n",
    "    return np.concatenate((signal, np.zeros(numZeros)), axis=None)\n",
    "\n",
    "def processMFCCs(signal, sr, windowSize, hopSize):\n",
    "    origLength = len(signal)\n",
    "    signal = padSignal(signal, sr, windowSize, hopSize)\n",
    "    mfccArray = np.empty((math.ceil(origLength/hopSize), 20))\n",
    "    block = 0\n",
    "    for i in range(0, origLength, hopSize):\n",
    "        toProcess = windowHann(signal[i: i+windowSize], sr, windowSize)\n",
    "        mfccArray[block, :] = python_speech_features.mfcc(toProcess, sr, winlen=(len(toProcess)/sr), nfft = len(toProcess), winstep=(len(toProcess)/sr), numcep=20)[0]\n",
    "        block+=1\n",
    "    return mfccArray\n",
    "\n",
    "# testName = trainFiles[2]\n",
    "# fName = os.path.join(trainDir, testName)\n",
    "# signal, sr = librosa.load(fName)\n",
    "# print(len(signal))\n",
    "# mfccArray = processWaveletMFCCs(signal, sr, windowSize, hopSize, levels[0], wavelets[0])\n",
    "# print(np.shape(mfccArray))\n",
    "\n",
    "\n",
    "fileName = \"MNIST_MFCC_Normal_20.pickle\"\n",
    "with open(fileName, 'wb') as handle:\n",
    "    for i in trainFiles:\n",
    "        fName = os.path.join(trainDir, i)\n",
    "        signal, sr = librosa.load(fName)\n",
    "        mfccArray = processMFCCs(signal, sr, windowSize, hopSize)\n",
    "        a = {i: mfccArray}\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Done with: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    arr = test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c735cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4da6f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mfccs = test[[*test][0]]\n",
    "print(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [*test][0]\n",
    "a = label.split('_')\n",
    "print(a[0], a[2].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[*i[[*i][0]]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc[[*label][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1522c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "list(chain.from_iterable(test[[*test][0]][[*label][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "testX = []\n",
    "testY = []\n",
    "for i in objects:\n",
    "    label = [*i][0].split('_')[0]\n",
    "    num = int([*i][0].split('_')[2].split('.')[0])\n",
    "    if num<=4:\n",
    "        testX.append(np.asarray(i[[*i][0]], dtype=np.float32))\n",
    "        testY.append(label)\n",
    "    else:\n",
    "        trainX.append(np.asarray(i[[*i][0]], dtype=np.float32))\n",
    "        trainY.append(label)\n",
    "#     trainX.append(list(chain.from_iterable(i[[*i][0]][[*i[[*i][0]]][0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9022ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainX), len(trainY), len(testX), len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dumpfileMNISTdataset.pickle', 'rb') as handle:\n",
    "    pickle.load(handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump({\"trainY\":trainY}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump({\"testX\":testX}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump({\"testY\":testY}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.asarray(trainX, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.asarray(trainY)\n",
    "testX = np.asarray(testX)\n",
    "testY = np.asarray(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainX)):\n",
    "    trainX[i] = np.pad(trainX[i], (0, 2561-len(trainX[i])), 'constant', constant_values=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0203e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlen = 0\n",
    "for i in range(len(trainX)):\n",
    "    trainX[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eaf47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = max(trainDataInput, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f4ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor.fit(trainX, trainY, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.asarray(trainY, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = []\n",
    "for i in trainDataOutput:\n",
    "    trainY.append(setDict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "for i in trainDataInput:\n",
    "    trainX.append(np.pad(np.asarray(i, dtype = np.float32), (0, 7815-len(i)), 'constant', constant_values=(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7dd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.asarray(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8af62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.asarray(trainY, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404baf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.reshape(trainX[0], (2561,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainX)):\n",
    "    trainX[i] = np.reshape(trainX[i], (2561,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd964d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (2700,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.reshape(trainY, (2700,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e106a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18e31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
